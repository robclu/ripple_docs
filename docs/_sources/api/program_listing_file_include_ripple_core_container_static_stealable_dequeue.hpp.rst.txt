
.. _program_listing_file_include_ripple_core_container_static_stealable_dequeue.hpp:

Program Listing for File static_stealable_dequeue.hpp
=====================================================

|exhale_lsh| :ref:`Return to documentation for file <file_include_ripple_core_container_static_stealable_dequeue.hpp>` (``include/ripple/core/container/static_stealable_dequeue.hpp``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   //==--- ripple/core/container/static_stealable_dequeue.hpp - -*- C++ -*- ---==//
   //
   //                                Ripple
   //
   //                      Copyright (c) 2020 Ripple
   //
   //  This file is distributed under the MIT License. See LICENSE for details.
   //
   //==------------------------------------------------------------------------==//
   //
   //
   //==------------------------------------------------------------------------==//
   
   #ifndef RIPPLE_CONTAINER_STATIC_STEALABLE_DEQUEUE_HPP
   #define RIPPLE_CONTAINER_STATIC_STEALABLE_DEQUEUE_HPP
   
   #include <array>
   #include <atomic>
   #include <optional>
   
   namespace ripple {
   
   template <typename Object, uint32_t MaxObjects>
   class alignas(avoid_false_sharing_size) StaticStealableDequeue {
    public:
   
     // clang-format off
     using size_type   = uint32_t;
     using atomic_t    = std::atomic<size_type>;
     using object_t    = Object;
     using optional_t  = std::optional<object_t>;
     using container_t = std::vector<object_t>;
     // clang-format on
   
    private:
     template <typename... Args>
     using pointer_enable_t =
       std::enable_if_t<(std::is_pointer_v<Args> * ... * true), int>;
   
     template <typename... Args>
     using non_pointer_enable_t =
       std::enable_if_t<(!std::is_pointer_v<Args> * ... * true), int>;
   
    public:
     //==--- [construction] ---------------------------------------------------==//
   
     StaticStealableDequeue() = default;
   
     StaticStealableDequeue(StaticStealableDequeue&& other) noexcept
     : _objects{std::move(other._objects)},
       _top{other._top.load()},
       _bottom{other._bottom.load()} {}
   
     StaticStealableDequeue(const StaticStealableDequeue&) = delete;
   
     //==--- [methods] --------------------------------------------------------==//
   
     template <typename T, pointer_enable_t<T> = 0>
     auto push(T ptr) noexcept -> void {
       const auto bottom = _bottom.load(std::memory_order_relaxed);
       const auto index  = wrapped_index(bottom);
   
       _objects[index] = ptr;
   
       // Ensure that the compiler does not reorder this instruction and the
       // setting of the object above, otherwise the object seems added (since the
       // bottom index has moved) but isn't (since it would not have been added).
       _bottom.store(bottom + 1, std::memory_order_release);
     }
   
     template <typename... Args, non_pointer_enable_t<Args...> = 0>
     auto push(Args&&... args) noexcept -> void {
       const auto bottom = _bottom.load(std::memory_order_relaxed);
       const auto index  = wrapped_index(bottom);
   
       new (&_objects[index]) object_t(std::forward<Args>(args)...);
   
       // Ensure that the compiler does not reorder this instruction and the
       // setting of the object above, otherwise the object seems added (since the
       // bottom index has moved) but isn't (since it would not have been added).
       _bottom.store(bottom + 1, std::memory_order_release);
     }
   
     template <typename... Args>
     auto try_push(Args&&... args) noexcept -> bool {
       if (size() >= MaxObjects) {
         return false;
       }
   
       push(std::forward<Args>(args)...);
       return true;
     }
   
     auto pop() noexcept -> optional_t {
       auto bottom = _bottom.load(std::memory_order_relaxed) - 1;
   
       // Sequentially consistant memory ordering is used here to ensure that the
       // load to top always happens after the load to bottom above, and that the
       // compiler emits an __mfence__ instruction.
       _bottom.store(bottom, std::memory_order_seq_cst);
   
       auto top = _top.load(std::memory_order_relaxed);
       if (top > bottom) {
         _bottom.store(top, std::memory_order_relaxed);
         return std::nullopt;
       }
   
       auto object = std::make_optional(_objects[wrapped_index(bottom)]);
       if (top != bottom) {
         return object;
       }
   
       // If we are here there is only one element left in the queue, and there may
       // be a race between this method and steal() to get it. If this exchange is
       // true then this method won the race (or was not in one) and then the
       // object can be returned, otherwise the queue has already been emptied.
       bool exchanged =
         _top.compare_exchange_strong(top, top + 1, std::memory_order_release);
   
       // This is also a little tricky: If we lost the race, top will be changed to
       // the new value set by the stealing thread (i.e it's already incremented).
       // If it's incremented again then _bottom > _top when the last item was
       // actually just cleared. This is also the unlikely case -- since this path
       // is only executed when there is contention on the last element -- so the
       // const of the branch is acceptable.
       _bottom.store(top + (exchanged ? 1 : 0), std::memory_order_relaxed);
   
       return exchanged ? object : std::nullopt;
     }
   
     auto steal() noexcept -> optional_t {
       auto top = _top.load(std::memory_order_relaxed);
   
       // Top must always be set before bottom, so that bottom - top represents an
       // accurate enough (to prevent error) view of the queue size. Loads to
       // different address aren't reordered (i.e load load barrier)
       asm volatile("" ::: "memory");
   
       auto bottom = _bottom.load(std::memory_order_relaxed);
       if (top >= bottom) {
         return std::nullopt;
       }
   
       // Here the object at top is fetched, and and update to _top is atempted,
       // since the top element is being stolen. __If__ the exchange succeeds,
       // then this method won a race with another thread to steal the element,
       // or if there is only a single element left, then it won a race between
       // other threads and the pop() method (potentially), or there was no race.
       // In summary, if the exchange succeeds, the object can be returned,
       // otherwise it can't.
       //
       // Also note that the object __must__ be created before the exchange to top,
       // otherwise there will potentially be a race to construct the object and
       // between a thread pushing onto the queue.
       //
       // This introduces overhead when there is contention to steal, and the steal
       // is unsuccessful, but in the succesful case there is no overhead.
       auto object = std::make_optional(_objects[wrapped_index(top)]);
       bool exchanged =
         _top.compare_exchange_strong(top, top + 1, std::memory_order_release);
   
       return exchanged ? object : std::nullopt;
     }
   
     auto size() const noexcept -> size_type {
       return _bottom.load(std::memory_order_relaxed) -
              _top.load(std::memory_order_relaxed);
     }
   
    private:
     //==--- [members] --------------------------------------------------------==//
   
     // Note: The starting values are 1 since the element popping pops (bottom - 1)
     //       so if bottom = 0 to start, then (0 - 1) = size_t_max, and the pop
     //       function tries to access an out of range element.
   
     container_t _objects{MaxObjects}; 
     atomic_t    _top    = 1;          
     atomic_t    _bottom = 1;          
   
     //==--- [methods] --------------------------------------------------------==//
   
     auto wrapped_index(size_type index) const noexcept -> size_type {
       return index % MaxObjects;
     }
   }; // namespace ripple
   
   } // namespace ripple
   
   #endif // RIPPLE_CONTAINER_STATIC_STEALABLE_DEQUEUE_HPP
